<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Posts on 李林克斯 </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://liyangliang.me/post/index.xml</link>
    <language>zh-CN</language>
    <author>Yangliang Li</author>
    
    <updated>Wed, 10 May 2017 17:48:17 CST</updated>
    
    <item>
      <title>Flask 应用国际化</title>
      <link>http://liyangliang.me/posts/2017/05/flask-i18n</link>
      <pubDate>Wed, 10 May 2017 17:48:17 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2017/05/flask-i18n</guid>
      <description>&lt;h2 id=&#34;babel&#34;&gt;Babel&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Babel is an integrated collection of utilities that assist in internationalizing and localizing Python applications, with an emphasis on web-based applications.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;文档：&lt;a href=&#34;http://babel.pocoo.org/en/latest/&#34;&gt;http://babel.pocoo.org/en/latest/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;代码：&lt;a href=&#34;https://github.com/python-babel/babel&#34;&gt;https://github.com/python-babel/babel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;flask-babel&#34;&gt;Flask-Babel&lt;/h2&gt;

&lt;p&gt;Flask 的 i18n 扩展，集成 babel、pytz 等。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文档：&lt;a href=&#34;https://pythonhosted.org/Flask-Babel/&#34;&gt;https://pythonhosted.org/Flask-Babel/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;代码：&lt;a href=&#34;https://github.com/python-babel/flask-babel&#34;&gt;https://github.com/python-babel/flask-babel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装：&lt;code&gt;pip install Flask-Babel&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;babel 配置文件：babel.cfg&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[python: **.py]
[jinja2: **.html]
extensions=jinja2.ext.autoescape,jinja2.ext.with_,webassets.ext.jinja2.AssetsExtension
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Flask-Babel 配置：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;BABEL_DEFAULT_LOCALE = &#39;zh_CN’                  # locale 选项，默认 &#39;en&#39;
BABEL_DEFAULT_TIMEZONE = &#39;Asia/Shanghai&#39;        # 时区，默认 &#39;UTC&#39;
BABEL_TRANSLATION_DIRECTORIES = &#39;translations&#39;  # 翻译文件所在目录，默认 &#39;translations&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;生成翻译文件模版：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;$ pybabel extract -F babel.cfg -o messages.pot .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果使用了 &lt;code&gt;lazy_gettext()&lt;/code&gt; 这样的函数，需要在上面的命令行参数指定：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pybabel extract -F babel.cfg -k lazy_gettext -o messages.pot .
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;生成翻译文件:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;$ pybabel init -i messages.pot -d translations
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;编辑 translations/zh_CN/LC_MESSAGES/messages.po 文件，手动翻译。po 文件内容形如：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;#: forms.py:65 forms.py:78
#: templates/flask_user/emails/invite_child_user_message.html:9
msgid &amp;quot;Username&amp;quot;
msgstr &amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中：
  - &lt;code&gt;#:&lt;/code&gt; 注释内容是 ‘文件名:行号’，即所有出现过的地方
  - &lt;code&gt;msgid&lt;/code&gt; 是需要翻译的内容
  - &lt;code&gt;msgstr&lt;/code&gt; 是翻译后的内容，如果留空，则会显示原文，即 msgid&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;更新翻译文件（一般只需要 init 一次）：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;$ pybabel update -i messages.pot -d translations
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;编译&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;$ pybabel compile -d translations
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;工作流&#34;&gt;工作流&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ pybabel extract -F babel.cfg -o messages.pot .
$ pybabel init -i messages.pot -d translations     # 第一次
$ pybabel update -i messages.pot -d translations   # 更新
# 手动翻译
$ pybabel compile -d translations
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>EC2 挂载 EBS</title>
      <link>http://liyangliang.me/posts/2017/05/mount-ebs-to-ec2</link>
      <pubDate>Wed, 10 May 2017 17:41:54 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2017/05/mount-ebs-to-ec2</guid>
      <description>&lt;p&gt;创建 EC2 实例的时候可以选择添加 EBS 卷，在实例运行后，需要手动挂载上去。&lt;/p&gt;

&lt;p&gt;详情见 &lt;a href=&#34;http://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/ebs-using-volumes.html&#34;&gt;EBS 的文档&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;用-lsblk-命令查看所有可用的磁盘及其安装点&#34;&gt;用 &lt;code&gt;lsblk&lt;/code&gt; 命令查看所有可用的磁盘及其安装点&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
`-xvda1 202:1    0   8G  0 part /
xvdb    202:16   0  30G  0 disk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 &lt;code&gt;xvda1&lt;/code&gt; 是根设备，挂载到了 &lt;code&gt;/&lt;/code&gt;；&lt;code&gt;xvdb&lt;/code&gt; 是刚才添加的 EBS 卷，还没有挂载。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;确定是否需要在卷上创建文件系统&#34;&gt;确定是否需要在卷上创建文件系统。&lt;/h2&gt;

&lt;p&gt;如果是新的 EBS，是一个原始的块存储设备，需要先创建文件系统才能安装使用。从快照还原的卷可能已经含有文件系统。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo file -s /dev/xvdb
/dev/xvdb: data

$ sudo file -s /dev/xvda1
/dev/xvda1: Linux rev 1.0 ext4 filesystem data, UUID=9fbb7c51-0409-4b50-ad40-068dcfe4bc89, volume name &amp;quot;cloudimg-rootfs&amp;quot; (needs journal recovery) (extents) (large files) (huge files)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 &lt;code&gt;/dev/xvdb&lt;/code&gt; 上面还没有文件系统，需要手动创建:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkfs -t ext4 /dev/xvdb
mke2fs 1.42.13 (17-May-2015)
Creating filesystem with 7864320 4k blocks and 1966080 inodes
Filesystem UUID: 2a0dae23-7b6e-42ec-95e1-df58f29520a4
Superblock backups stored on blocks:
     32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,
     4096000

Allocating group tables: done
Writing inode tables: done
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done

$ sudo file -s /dev/xvdb
/dev/xvdb: Linux rev 1.0 ext4 filesystem data, UUID=2a0dae23-7b6e-42ec-95e1-df58f29520a4 (extents) (large files) (huge files)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;code&gt;mkfs&lt;/code&gt; 会格式化卷，删除所有数据。&lt;/p&gt;

&lt;h2 id=&#34;创建安装点-也就是要挂载的位置&#34;&gt;创建安装点，也就是要挂载的位置:&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir /data
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;挂载&#34;&gt;挂载&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;sudo mount /dev/xvdb /data/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;用-df-命令磁盘空间&#34;&gt;用 &lt;code&gt;df&lt;/code&gt; 命令磁盘空间&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            489M     0  489M   0% /dev
tmpfs           100M  3.1M   97M   4% /run
/dev/xvda1      7.8G  1.9G  5.5G  26% /
tmpfs           496M     0  496M   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           496M     0  496M   0% /sys/fs/cgroup
tmpfs           100M     0  100M   0% /run/user/1000
/dev/xvdb        30G   44M   28G   1% /data
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>MySQL 数据导入到 Redshift</title>
      <link>http://liyangliang.me/posts/2017/05/transfer-data-from-mysql-to-redshift</link>
      <pubDate>Wed, 10 May 2017 17:36:29 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2017/05/transfer-data-from-mysql-to-redshift</guid>
      <description>&lt;h2 id=&#34;设计表&#34;&gt;设计表&lt;/h2&gt;

&lt;p&gt;首先是设计表结构。建表语法差别不大，有一些地方可以注意一下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Redshift 貌似没有无符号类型，所以要把 unsigned 类型的字段修改成相应的 INT 或 BIGINT 类型。&lt;/li&gt;
&lt;li&gt;FLOAT 类型改成 REAL 或 FLOAT4&lt;/li&gt;
&lt;li&gt;把索引语句去掉，保留主键、外键、唯一性约束，Redshift 不会检查这些约束，但是查询时会用于优化。&lt;/li&gt;
&lt;li&gt;Redshift 的 CHAR 类型只能包含单字节 ASCII 字符，对于非 ASCII 数据需要把 CHAR 改成 VARCHAR 类型&lt;/li&gt;
&lt;li&gt;有可能 MySQL 中存的是 unicode，而 Redshift 中存的是 bytes，所以 VARCHAR 的长度也要调整，避免溢出。最简单的，可以用 MySQL 的字段长度 * 3.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于 sort key, dist key 等设计，只属于 Redshift 范畴，参考官网文档即可。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;加载数据&#34;&gt;加载数据&lt;/h2&gt;

&lt;p&gt;因为 Redshift 推荐使用 &lt;code&gt;COPY&lt;/code&gt; 命令从 S3 加载数据，所以首先得要从 MySQL 中导出数据，然后上传到 CSV.&lt;/p&gt;

&lt;p&gt;以导出 CSV 为例，需要注意使用 &lt;code&gt;&amp;quot;&lt;/code&gt; 符号作为转义字符，而不是 &lt;code&gt;\&lt;/code&gt;. 另外最好用 &lt;code&gt;&amp;quot;&lt;/code&gt; 把每个值都 wrap 起来，免得有些多行字符串导致出错。导出后可以压缩成 gzip 格式，在上传 S3 的时候可以快一些。&lt;/p&gt;

&lt;p&gt;Redshift 的 &lt;code&gt;COPY&lt;/code&gt; 例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;COPY syns_bigdata
FROM &#39;s3://some-bucket/some_filename.csv.gz&#39;
credentials &#39;aws_access_key_id=&amp;lt;aws_access_key_id&amp;gt;;aws_secret_access_key=&amp;lt;aws_secret_access_key&amp;gt;&#39;
region &#39;cn-north-1&#39; CSV GZIP NULL AS &#39;NULL&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;语法很简单，需要注意的有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;aws_access_key_id&lt;/code&gt; 和 &lt;code&gt;aws_secret_access_key&lt;/code&gt; 要有访问 S3 的权限&lt;/li&gt;
&lt;li&gt;指定 region&lt;/li&gt;
&lt;li&gt;指定文件格式，&lt;code&gt;CSV GZIP&lt;/code&gt; 表示是 gzip 压缩的 CSV 文件&lt;/li&gt;
&lt;li&gt;可以用 &lt;code&gt;NULL AS&lt;/code&gt; 语句指定 NULL 值&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>在 AWS 上安装 Tableau Server</title>
      <link>http://liyangliang.me/posts/2017/05/install-tableau-server-on-aws-ec2</link>
      <pubDate>Wed, 10 May 2017 17:23:55 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2017/05/install-tableau-server-on-aws-ec2</guid>
      <description>&lt;h2 id=&#34;启动-ec2-实例&#34;&gt;启动 EC2 实例&lt;/h2&gt;

&lt;p&gt;先根据 Tableau Server 的使用情况确定需要的配置，从而确定实例类型。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AMI: Microsoft Windows Server 2012 R2 Base（简体中文）&lt;/li&gt;
&lt;li&gt;类型: m4.4xlarge&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;启动、配置步骤略去不表，有两点需要注意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VPC 需要开启 3389 端口用于远程登录（RDP）&lt;/li&gt;
&lt;li&gt;密钥对会用于解密登录密码&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装-tableau-server&#34;&gt;安装 Tableau Server&lt;/h2&gt;

&lt;p&gt;从 Tableau 官网下载然后安装，配置、激活过程比较简单，略去不表。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;可选-安装-mysql-驱动&#34;&gt;（可选）安装 MySQL 驱动&lt;/h2&gt;

&lt;p&gt;在 &lt;a href=&#34;https://www.tableau.com/zh-cn/support/drivers&#34;&gt;这个页面&lt;/a&gt;可以找到所有数据源需要的驱动程序.&lt;/p&gt;

&lt;p&gt;下载好驱动程序，如 mysql-connector-odbc-5.3.7-winx64.msi，双击安装，提示错误。搜索了一番，应该是缺少 Visual C++ 的运行库。试过 Visual C++ Redistributable for Visual Studio 2012 Update 4 和 Visual C++ Redistributable Packages for Visual Studio 2013，最后发现后者才有用。&lt;/p&gt;

&lt;p&gt;安装完 &lt;a href=&#34;https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=40784&#34;&gt;Visual C++ Redistributable Packages for Visual Studio 2013&lt;/a&gt; 之后，可以成功安装mysql-connector-odbc-5.3.7-winx64.msi 。&lt;/p&gt;

&lt;h2 id=&#34;安装-aws-命令行程序&#34;&gt;安装 AWS 命令行程序&lt;/h2&gt;

&lt;p&gt;从这里下载：&lt;a href=&#34;https://s3.amazonaws.com/aws-cli/AWSCLI64.msi&#34;&gt;https://s3.amazonaws.com/aws-cli/AWSCLI64.msi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装完后打开 cmd，运行 &lt;code&gt;aws configure&lt;/code&gt; 进行配置，要有上传 S3 的权限。完成后可以运行 &lt;code&gt;aws s3 ls&lt;/code&gt; 验证。&lt;/p&gt;

&lt;h2 id=&#34;编写备份脚本&#34;&gt;编写备份脚本&lt;/h2&gt;

&lt;p&gt;自动备份并且把备份文件上传到 S3。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@echo OFF
set Binpath=&amp;quot;C:\Program Files\Tableau\Tableau Server\10.2\bin&amp;quot;
set Backuppath=&amp;quot;C:\Backups\Tablea Server\nightly&amp;quot;
echo %date% %time%: *** Housekeeping started ***

rmdir %Backuppath% /S /Q

%Binpath%\tabadmin backup %Backuppath%\ts_backup -d --no-config
timeout 5

%Binpath%\tabadmin cleanup

echo %date% %time%: Uploading to S3

aws s3 cp %Backuppath% s3://marspet-tableau-backup/ --recursive --exclude &amp;quot;*&amp;quot; --include &amp;quot;ts_backup-*.tsbak&amp;quot;

echo %date% %time%: *** Housekeeping completed ***
timeout 5
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;从备份恢复&#34;&gt;从备份恢复&lt;/h2&gt;

&lt;p&gt;如果是从其他的 Tableau Server 迁移过来，可以使用备份文件迁移数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Users\Administrator&amp;gt;&amp;quot;C:\Program Files\Tableau\Tableau Server\10.2\bin\tabadmi
n.bat&amp;quot; restore --no-config Downloads\ts_backup-2017-04-05.tsbak
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;restore 操作会关闭 Tableau Server，恢复完成后需要手动开启。&lt;/p&gt;

&lt;h2 id=&#34;自动备份&#34;&gt;自动备份&lt;/h2&gt;

&lt;p&gt;使用 Task Scheduler 实现，详情见官方文档：&lt;a href=&#34;http://technet.microsoft.com/en-us/library/cc766428.aspx&#34;&gt;http://technet.microsoft.com/en-us/library/cc766428.aspx&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Python 多进程导入数据到 MySQL</title>
      <link>http://liyangliang.me/posts/2017/02/load-data-into-mysql-using-python-multiprocessing</link>
      <pubDate>Sat, 25 Feb 2017 16:16:14 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2017/02/load-data-into-mysql-using-python-multiprocessing</guid>
      <description>&lt;p&gt;前段时间帮同事处理了一个把 CSV 数据导入到 MySQL 的需求。两个很大的 CSV 文件，
分别有 3GB、2100 万条记录和 7GB、3500 万条记录。对于这个量级的数据，用简单的单进程／单线程导入
会耗时很久，最终用了多进程的方式来实现。具体过程不赘述，记录一下几个要点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;批量插入而不是逐条插入&lt;/li&gt;
&lt;li&gt;为了加快插入速度，先不要建索引&lt;/li&gt;
&lt;li&gt;生产者和消费者模型，主进程读文件，多个 worker 进程执行插入&lt;/li&gt;
&lt;li&gt;注意控制 worker 的数量，避免对 MySQL 造成太大的压力&lt;/li&gt;
&lt;li&gt;注意处理脏数据导致的异常&lt;/li&gt;
&lt;li&gt;原始数据是 GBK 编码，所以还要注意转换成 UTF-8&lt;/li&gt;
&lt;li&gt;用 &lt;a href=&#34;http://click.pocoo.org/5/&#34;&gt;click&lt;/a&gt; 封装命令行工具&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的代码实现如下：&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import codecs
import csv
import logging
import multiprocessing
import os
import warnings

import click
import MySQLdb
import sqlalchemy

warnings.filterwarnings(&#39;ignore&#39;, category=MySQLdb.Warning)

# 批量插入的记录数量
BATCH = 5000

DB_URI = &#39;mysql://root@localhost:3306/example?charset=utf8&#39;

engine = sqlalchemy.create_engine(DB_URI)


def get_table_cols(table):
    sql = &#39;SELECT * FROM `{table}` LIMIT 0&#39;.format(table=table)
    res = engine.execute(sql)
    return res.keys()


def insert_many(table, cols, rows, cursor):
    sql = &#39;INSERT INTO `{table}` ({cols}) VALUES ({marks})&#39;.format(
            table=table,
            cols=&#39;, &#39;.join(cols),
            marks=&#39;, &#39;.join([&#39;%s&#39;] * len(cols)))
    cursor.execute(sql, *rows)
    logging.info(&#39;process %s inserted %s rows into table %s&#39;, os.getpid(), len(rows), table)


def insert_worker(table, cols, queue):
    rows = []
    # 每个子进程创建自己的 engine 对象
    cursor = sqlalchemy.create_engine(DB_URI)
    while True:
        row = queue.get()
        if row is None:
            if rows:
                insert_many(table, cols, rows, cursor)
            break

        rows.append(row)
        if len(rows) == BATCH:
            insert_many(table, cols, rows, cursor)
            rows = []


def insert_parallel(table, reader, w=10):
    cols = get_table_cols(table)

    # 数据队列，主进程读文件并往里写数据，worker 进程从队列读数据
    # 注意一下控制队列的大小，避免消费太慢导致堆积太多数据，占用过多内存
    queue = multiprocessing.Queue(maxsize=w*BATCH*2)
    workers = []
    for i in range(w):
        p = multiprocessing.Process(target=insert_worker, args=(table, cols, queue))
        p.start()
        workers.append(p)
        logging.info(&#39;starting # %s worker process, pid: %s...&#39;, i + 1, p.pid)

    dirty_data_file = &#39;./{}_dirty_rows.csv&#39;.format(table)
    xf = open(dirty_data_file, &#39;w&#39;)
    writer = csv.writer(xf, delimiter=reader.dialect.delimiter)

    for line in reader:
        # 记录并跳过脏数据: 键值数量不一致
        if len(line) != len(cols):
            writer.writerow(line)
            continue

        # 把 None 值替换为 &#39;NULL&#39;
        clean_line = [None if x == &#39;NULL&#39; else x for x in line]

        # 往队列里写数据
        queue.put(tuple(clean_line))
        if reader.line_num % 500000 == 0:
            logging.info(&#39;put %s tasks into queue.&#39;, reader.line_num)

    xf.close()

    # 给每个 worker 发送任务结束的信号
    logging.info(&#39;send close signal to worker processes&#39;)
    for i in range(w):
        queue.put(None)

    for p in workers:
        p.join()


def convert_file_to_utf8(f, rv_file=None):
    if not rv_file:
        name, ext = os.path.splitext(f)
        if isinstance(name, unicode):
            name = name.encode(&#39;utf8&#39;)
        rv_file = &#39;{}_utf8{}&#39;.format(name, ext)
    logging.info(&#39;start to process file %s&#39;, f)
    with open(f) as infd:
        with open(rv_file, &#39;w&#39;) as outfd:
            lines = []
            loop = 0
            chunck = 200000
            first_line = infd.readline().strip(codecs.BOM_UTF8).strip() + &#39;\n&#39;
            lines.append(first_line)
            for line in infd:
                clean_line = line.decode(&#39;gb18030&#39;).encode(&#39;utf8&#39;)
                clean_line = clean_line.rstrip() + &#39;\n&#39;
                lines.append(clean_line)
                if len(lines) == chunck:
                    outfd.writelines(lines)
                    lines = []
                    loop += 1
                    logging.info(&#39;processed %s lines.&#39;, loop * chunck)

            outfd.writelines(lines)
            logging.info(&#39;processed %s lines.&#39;, loop * chunck + len(lines))


@click.group()
def cli():
    logging.basicConfig(level=logging.INFO,
                        format=&#39;%(asctime)s - %(levelname)s - %(name)s - %(message)s&#39;)


@cli.command(&#39;gbk_to_utf8&#39;)
@click.argument(&#39;f&#39;)
def convert_gbk_to_utf8(f):
    convert_file_to_utf8(f)


@cli.command(&#39;load&#39;)
@click.option(&#39;-t&#39;, &#39;--table&#39;, required=True, help=&#39;表名&#39;)
@click.option(&#39;-i&#39;, &#39;--filename&#39;, required=True, help=&#39;输入文件&#39;)
@click.option(&#39;-w&#39;, &#39;--workers&#39;, default=10, help=&#39;worker 数量，默认 10&#39;)
def load_fac_day_pro_nos_sal_table(table, filename, workers):
    with open(filename) as fd:
        fd.readline()   # skip header
        reader = csv.reader(fd)
        insert_parallel(table, reader, w=workers)


if __name__ == &#39;__main__&#39;:
    cli()
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>在 Flask 项目的 celery 中使用 gevent</title>
      <link>http://liyangliang.me/posts/2016/05/using-celery-with-flask-and-gevent</link>
      <pubDate>Tue, 17 May 2016 16:42:37 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2016/05/using-celery-with-flask-and-gevent</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://liyangliang.me/posts/2015/11/using-celery-with-flask/&#34;&gt;在 Flask 项目中使用 Celery&lt;/a&gt; 这篇文章谈到了如何在 Flask 项目中集成 Celery，也讲了在 celery 任务中引用 Flask 的 application context 的方法。一般情况下那样使用是没问题的，但是如果需要在 task 中使用 gevent，就需要一些额外的改进。至少有两点。&lt;/p&gt;

&lt;h2 id=&#34;1-使用-gevent-并发模型&#34;&gt;1. 使用 gevent 并发模型&lt;/h2&gt;

&lt;p&gt;如果在 task 中要使用 gevent，就必须使用 gevent 并发模型。这很好处理，只需要修改启动选项就行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ celery worker -A celery_worker.celery -P gevent -c 10 -l INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的命令，&lt;code&gt;-P&lt;/code&gt; 选项指定 pool，默认是 prefork，这里是 gevent; &lt;code&gt;-c&lt;/code&gt; 设置并发数。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-引用-flask-的-application-context&#34;&gt;2. 引用 Flask 的 application context&lt;/h2&gt;

&lt;p&gt;这个问题也是在 &lt;a href=&#34;http://liyangliang.me/posts/2015/11/using-celery-with-flask/&#34;&gt;在 Flask 项目中使用 Celery&lt;/a&gt; 中重点讨论的，在这种场景下，上文的解决方法起不到作用，仍然会报错（具体原因不太懂，知道的朋友请不吝赐教）。解决方案就是，把需要引用 Flask app 的地方（如 app.config），放到 Flask 的 application context 里执行，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with app.app_context():
    print app.config.get(&#39;SOME_CONFIG_KEY&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在实际应用中，我最后写了个装饰器来实现这个目的。简单介绍一下场景，项目用到了 Flask-Cache，项目启动时会创建全局单例 &lt;code&gt;cache&lt;/code&gt;，并在 &lt;code&gt;create_app&lt;/code&gt; 中进行初始化。在 Flask-Cache 初始化时，会把当前的 Flask app 对象绑定到实例 &lt;code&gt;cache&lt;/code&gt; 中，所以可以尝试从这里获取 app 对象。&lt;/p&gt;

&lt;p&gt;代码的目录结构与之前一样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── README.md
├── app
│   ├── __init__.py
│   ├── config.py
│   ├── forms
│   ├── models
│   ├── tasks
│   │   ├── __init__.py
│   │   └── email.py
│   └── views
│   │   ├── __init__.py
│   │   └── account.py
├── celery_worker.py
├── manage.py
└── wsgi.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;装饰器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def with_app_context(task):
    memo = {&#39;app&#39;: None}

    @functools.wraps(task)
    def _wrapper(*args, **kwargs):
        if not memo[&#39;app&#39;]:
            try:
                # 尝试从 cache 中获取 app 对象，如果得到的不是 None，就不需要重复创建了
                app = cache.app
                _ = app.name
            except Exception:
                from app import create_app

                app = create_app()
            memo[&#39;app&#39;] = app
        else:
            app = memo[&#39;app&#39;]

        # 把 task 放到 application context 环境中运行
        with app.app_context():
            return task(*args, **kwargs)

    return _wrapper
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@celery.task()
@with_app_context
def add(x, y):
    print app.config.get(&#39;SOME_CONFIG_KEY&#39;)
    return x + y
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>MongoDB Replica Set 重新同步</title>
      <link>http://liyangliang.me/posts/2016/04/mongodb-replica-set-resync</link>
      <pubDate>Fri, 15 Apr 2016 11:47:00 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2016/04/mongodb-replica-set-resync</guid>
      <description>&lt;p&gt;生产环境上用了 MongoDB，三个节点组成的 ReplicaSet（复制集）。部署好后，应用一直没出过问题，所以平时也没管过。今天早上突然想上服务器看看，于是登录了 primary 节点查看日志，发现这条日志不断重复：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2016-04-15T03:02:39.470+0000 W NETWORK  [ReplExecNetThread-28676] Failed to connect to 172.31.168.48:11102, reason: errno:111 Connection refused
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实就是有个 secondary 节点一直连接不上。不太可能是网络问题，所以很可能是那个节点的 mongod 进程挂掉了。登录上 secondary 节点，mongod 进程果然不在运行；查看日志发现最后一条是在 2016-03-21. 一时间有两个疑问涌上心头：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;为什么会挂掉？&lt;/li&gt;
&lt;li&gt;如何修复？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;当务之急是先修复集群，这一点官方文档有说明：&lt;a href=&#34;https://docs.mongodb.org/manual/tutorial/resync-replica-set-member/&#34;&gt;Resync a Member of a Replica Set&lt;/a&gt;. 其实就是删除数据文件，然后通过 initial sync 来重新同步。有两种 initial sync 的方式：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;清空数据目录，重启 mongod 实例，让MongoDB进行正常的初始化同步。这是个简单的方式，但是耗时较长。&lt;/li&gt;
&lt;li&gt;为该机器从其他节点上复制一份最近的数据文件，并重启。操作步骤较多，但是最为快速。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;考虑到数据量并没有很多，所以决定使用第一种比较简单的方式。重启好后，发现数据目录很快就新建了很多文件。和 primary 节点对比，文件名和大小均一致；primary 节点和另一个 secondary 节点也不再出现连接失败的日志。&lt;/p&gt;

&lt;p&gt;遗憾的是，挂掉的原因却一直没有找到。日志文件里没有发现异常，&lt;code&gt;history&lt;/code&gt; 也没发现有 &lt;code&gt;kill&lt;/code&gt; 的记录。
幸运的是，集群很快就恢复了，应用不受影响，数据也没丢失。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Nginx AWS ELB 域名解析</title>
      <link>http://liyangliang.me/posts/2016/04/nginx-aws-elb-name-resolution</link>
      <pubDate>Thu, 14 Apr 2016 15:33:52 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2016/04/nginx-aws-elb-name-resolution</guid>
      <description>&lt;p&gt;最近生产环境上出现了一个奇怪的问题。某日下午，APP 向某个域名发出的所有请求没有响应，服务端也没收到请求；而向另一个域名的请求却没有问题。先记录一下背景：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;两个域名：api.example.com, web.example.com&lt;/li&gt;
&lt;li&gt;环境：AWS + ELB + Nginx&lt;/li&gt;
&lt;li&gt;后端：Python + Django + Gunicorn&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;出问题的是 api.example.com （下文简称 API）这个域名，所以 web.example.com 就不细说。由于一些历史原因，API 的请求链路大概是这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                      proxy_pass         backends                      proxy_pass
APP -----&amp;gt; API Nginx -------------&amp;gt; ELB -----------&amp;gt; Backend Nginx(s) ------------&amp;gt; Gunicorn(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 API 的 Nginx 配置大概是这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;location /test {
    proxy_pass http://name.of.elb.aws.com;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;文章开头描述的现象就是，在 API 的 Nginx 能看到 access log，但是 Backend 的 Nginx 没有接收到请求。所以问题可能出在代理这一步。奇怪的地方在于，刚上线时一切正常，运行了一段时间后才突然出现。猜测有可能是 DNS 解析的问题，但没有根据，也不知道如何解决。&lt;/p&gt;

&lt;p&gt;后来 Google 了一番，发现确实是 DNS 的问题。Nginx 会在启动的时候进行域名查找，然后把 IP 地址缓存起来，后续就直接使用这些 IP 地址。而 AWS 的 ELB 所指向的 IP 地址是不固定的，会经常更新；所以这会导致 Nginx 缓存的 IP 地址实际上已经失效。定位出了问题，也参考网上的做法，把 API 的 Nginx 配置稍作修改：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;location /test {
    resolver 233.5.5.5 valid=30s;
    proxy_pass http://name.of.elb.aws.com;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 &lt;a href=&#34;http://nginx.org/en/docs/http/ngx_http_core_module.html#resolver&#34;&gt;resolver&lt;/a&gt; 就是 Nginx 用于把域名转换为 IP 地址的域名服务器。后面的第一个参数是域名服务器，valid 指定了缓存有效期，这里是 30s （默认 5min）. 加上这个配置后，Nginx 会用指定的域名服务器来解析域名，并定期把缓存失效。这样就能避免 ELB 地址更新带来的问题。&lt;/p&gt;

&lt;p&gt;刚开始以为只需要加上 resolver 这一行配置就可以，后来看 &lt;a href=&#34;[http://serverfault.com/a/562518/192152&#34;&gt;这个 serverfault 上的回答&lt;/a&gt;，还需要把 proxy_pass 的地址定义成一个变量。于是最终的配置变成了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;location /test {
    resolver 233.5.5.5 valid=30s;
    set $backends &amp;quot;http://name.of.elb.aws.com&amp;quot;;
    proxy_pass $backends;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置，reload，一两天后如果无响应的现象不再出现，说明问题已经解决。&lt;/p&gt;

&lt;p&gt;参考材料：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://nginx.org/en/docs/http/ngx_http_core_module.html#resolver&#34;&gt;Nginx 文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tenzer.dk/nginx-with-dynamic-upstreams/&#34;&gt;Nginx with dynamic upstreams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://gc-taylor.com/blog/2011/11/10/nginx-aws-elb-name-resolution-resolvers&#34;&gt;nginx AWS ELB name resolution with resolvers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://serverfault.com/questions/560632/some-nginx-reverse-proxy-configs-stops-working-once-a-day&#34;&gt;serverfault - Some nginx reverse proxy configs stops working once a day&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>zhihu-go 源码解析：用 goquery 解析 HTML</title>
      <link>http://liyangliang.me/posts/2016/03/zhihu-go-insight-parsing-html-with-goquery</link>
      <pubDate>Wed, 30 Mar 2016 23:02:51 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2016/03/zhihu-go-insight-parsing-html-with-goquery</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://liyangliang.me/posts/2016/03/zhihu-go/&#34;&gt;上一篇博客&lt;/a&gt; 简单介绍了 &lt;a href=&#34;https://github.com/DeanThompson/zhihu-go&#34;&gt;zhihu-go&lt;/a&gt; 项目的缘起，本篇简单介绍一下关于处理 HTML 的细节。&lt;/p&gt;

&lt;p&gt;因为知乎没有开发 API，所以只能通过模拟浏览器操作的方式获取数据，这些数据有两种格式：普通的 HTML 文档和某些 Ajax 接口返回的 JSON（返回的数据实际上也是 HTML）。其实也就是爬虫了，抓取网页，然后提取数据。一般来说从 HTML 文档提取数据有这些做法：正则、XPath、CSS 选择器等。对我来说，正则写起来比较复杂，代码可读性差而且维护起来麻烦；XPath 没有详细了解，不过用起来应该不难，而且 Chrome 浏览器可以直接提取 XPath. zhihu-go 里用的是选择器的方式，使用了 &lt;a href=&#34;https://github.com/PuerkitoBio/goquery&#34;&gt;goquery&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;goquery 是 &amp;ldquo;a little like that j-thing, only in Go&amp;rdquo;，也就是用 jQuery 的方式去操作 DOM. jQuery 大家都很熟，API 也很简单明了。本文不详细介绍 goquery，下面选几个场景（API）讲讲在 zhihu-go 里的应用。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建-document-对象&#34;&gt;创建 Document 对象&lt;/h3&gt;

&lt;p&gt;goquery 暴露了两个结构体：&lt;code&gt;Document&lt;/code&gt; 和 &lt;code&gt;Selection&lt;/code&gt;. &lt;code&gt;Document&lt;/code&gt; 表示一个 HTML 文档，&lt;code&gt;Selection&lt;/code&gt; 用于像 jQuery 一样操作，支持链式调用。goquery 需要指定一个 HTML 文档才能继续后续的操作，有以下几个构造方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NewDocumentFromNode(root *html.Node) *Document&lt;/code&gt;: 传入 &lt;code&gt;*html.Node&lt;/code&gt; 对象，也就是根节点。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NewDocument(url string) (*Document, error)&lt;/code&gt;: 传入 URL，内部用 &lt;code&gt;http.Get&lt;/code&gt; 获取网页。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NewDocumentFromReader(r io.Reader) (*Document, error)&lt;/code&gt;: 传入 &lt;code&gt;io.Reader&lt;/code&gt;，内部从 reader 中读取内容并解析。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NewDocumentFromResponse(res *http.Response) (*Document, error)&lt;/code&gt;: 传入 HTTP 响应，内部拿到 &lt;code&gt;res.Body&lt;/code&gt;(实现了 &lt;code&gt;io.Reader&lt;/code&gt;) 后的处理方式类似 &lt;code&gt;NewDocumentFromReader&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为知乎的页面需要登录才能访问（还需要伪造请求头），而且我们并不想手动解析 HTML 来获取 &lt;code&gt;*html.Node&lt;/code&gt;，最后用到了另外两个构造方法。大致的使用场景是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求 HTML 页面（如问题页面），调用 &lt;code&gt;NewDocumentFromResponse&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;请求 Ajax 接口，返回的 JSON 数据里是一些 HTML 片段，用 &lt;code&gt;NewDocumentFromReader&lt;/code&gt;，其中 &lt;code&gt;r = strings.NewReader(html)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了方便举例说明，下文采用这个定义: &lt;code&gt;var doc *goquery.Document&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;查找到指定节点&#34;&gt;查找到指定节点&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;Selection&lt;/code&gt; 有一系列类似 jQuery 的方法，&lt;code&gt;Document&lt;/code&gt; 结构体内嵌了 &lt;code&gt;*Selection&lt;/code&gt;，因此也能直接调用这些方法。主要的方法是 &lt;code&gt;Selection.Find(selector string)&lt;/code&gt;，传入一个选择器，返回一个新的，匹配到的 &lt;code&gt;*Selection&lt;/code&gt;，所以能够链式调用。&lt;/p&gt;

&lt;p&gt;比如在用户主页（如 &lt;a href=&#34;https://www.zhihu.com/people/jixin&#34;&gt;黄继新&lt;/a&gt;），要获取用户的 BIO. 首先用 Chrome 定位到对应的 HTML：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;span class=&amp;quot;bio&amp;quot; title=&amp;quot;和知乎在一起&amp;quot;&amp;gt;和知乎在一起&amp;lt;/span&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对应的 go 代码就是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;doc.Find(&amp;quot;span.bio&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果一个选择器对应多个结果，可以使用 &lt;code&gt;First()&lt;/code&gt;, &lt;code&gt;Last()&lt;/code&gt;, &lt;code&gt;Eq(index int)&lt;/code&gt;, &lt;code&gt;Slice(start, end int)&lt;/code&gt; 这些方法进一步定位。&lt;/p&gt;

&lt;p&gt;还是在用户主页，在用户资料栏的底下，从左往右展示了提问数、回答数、文章数、收藏数和公共编辑的次数。查看 HTML 源码后发现这几项的 class 是一样的，所以只能通过下标索引来区分。&lt;/p&gt;

&lt;p&gt;先看 HTML 源码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;div class=&amp;quot;profile-navbar clearfix&amp;quot;&amp;gt;
&amp;lt;a class=&amp;quot;item &amp;quot; href=&amp;quot;/people/jixin/asks&amp;quot;&amp;gt;提问&amp;lt;span class=&amp;quot;num&amp;quot;&amp;gt;1336&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;a class=&amp;quot;item &amp;quot; href=&amp;quot;/people/jixin/answers&amp;quot;&amp;gt;回答&amp;lt;span class=&amp;quot;num&amp;quot;&amp;gt;785&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;a class=&amp;quot;item &amp;quot; href=&amp;quot;/people/jixin/posts&amp;quot;&amp;gt;文章&amp;lt;span class=&amp;quot;num&amp;quot;&amp;gt;91&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;a class=&amp;quot;item &amp;quot; href=&amp;quot;/people/jixin/collections&amp;quot;&amp;gt;收藏&amp;lt;span class=&amp;quot;num&amp;quot;&amp;gt;44&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;a class=&amp;quot;item &amp;quot; href=&amp;quot;/people/jixin/logs&amp;quot;&amp;gt;公共编辑&amp;lt;span class=&amp;quot;num&amp;quot;&amp;gt;51648&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果要定位找到回答数，对应的 go 代码是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;doc.Find(&amp;quot;div.profile-navbar&amp;quot;).Find(&amp;quot;span.num&amp;quot;).Eq(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;属性操作&#34;&gt;属性操作&lt;/h3&gt;

&lt;p&gt;经常需要获取一个标签的内容和某些属性值，使用 goquery 可以很容易做到。&lt;/p&gt;

&lt;p&gt;继续上面获取回答数的例子，用 &lt;code&gt;Text() string&lt;/code&gt; 方法可以获取标签内的文本内容，其中包含所有子标签。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;text := doc.Find(&amp;quot;div.profile-navbar&amp;quot;).Find(&amp;quot;span.num&amp;quot;).Eq(1).Text()    // &amp;quot;785&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要注意的是，&lt;code&gt;Text()&lt;/code&gt; 方法返回的字符串，可能前后有很多空白字符，可以视情况做清除。&lt;/p&gt;

&lt;p&gt;获取属性值也很容易，有两个方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Attr(attrName string) (val string, exists bool)&lt;/code&gt;: 返回属性值和该属性是否存在，类似从 &lt;code&gt;map&lt;/code&gt; 中取值&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AttrOr(attrName, defaultValue string) string&lt;/code&gt;: 和上一个方法类似，区别在于如果属性不存在，则返回给定的默认值&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;常见的使用场景就是获取一个 a 标签的链接。继续上面获取回答的例子，如果想要得到用户回答的主页，可以这么做：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;href, _ := doc.Find(&amp;quot;div.profile-navbar&amp;quot;).Find(&amp;quot;a.item&amp;quot;).Eq(1).Attr(&amp;quot;href&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有其他设置属性、操作 class 的方法，就不展开讨论了。&lt;/p&gt;

&lt;h3 id=&#34;迭代&#34;&gt;迭代&lt;/h3&gt;

&lt;p&gt;很多场景需要返回列表数据，比如问题的关注者列表、所有回答，某个答案的点赞的用户列表等。这种情况下一般需要用到迭代，遍历所有的同类节点，做某些操作。&lt;/p&gt;

&lt;p&gt;goquery 提供了三个用于迭代的方法，都接受一个匿名函数作为参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Each(f func(int, *Selection)) *Selection&lt;/code&gt;: 其中函数 &lt;code&gt;f&lt;/code&gt; 的第一个参数是当前的下标，第二个参数是当前的节点&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EachWithBreak(f func(int, *Selection) bool) *Selection&lt;/code&gt;: 和 &lt;code&gt;Each&lt;/code&gt; 类似，增加了中途跳出循环的能力，当 &lt;code&gt;f&lt;/code&gt; 返回 &lt;code&gt;false&lt;/code&gt; 时结束迭代&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Map(f func(int, *Selection) string) (result []string)&lt;/code&gt;: &lt;code&gt;f&lt;/code&gt; 的参数与上面一样，返回一个 string 类型，最终返回 []string.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如获取一个收藏夹（如 &lt;a href=&#34;https://www.zhihu.com/collection/19573315&#34;&gt;黄继新的收藏：关于知乎的思考&lt;/a&gt;）下所有的问题，可以这么做（见 &lt;a href=&#34;https://github.com/DeanThompson/zhihu-go/blob/master/collection.go&#34;&gt;zhihu-go/collections.go&lt;/a&gt;）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func getQuestionsFromDoc(doc *goquery.Document) []*Question {
	questions := make([]*Question, 0, pageSize)
	items := doc.Find(&amp;quot;div#zh-list-answer-wrap&amp;quot;).Find(&amp;quot;h2.zm-item-title&amp;quot;)
	items.Each(func(index int, sel *goquery.Selection) {
		a := sel.Find(&amp;quot;a&amp;quot;)
		qTitle := strip(a.Text())
		qHref, _ := a.Attr(&amp;quot;href&amp;quot;)
		thisQuestion := NewQuestion(makeZhihuLink(qHref), qTitle)
		questions = append(questions, thisQuestion)
	})
	return questions
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;EachWithBreak&lt;/code&gt; 在 zhihu-go 中也有用到，可以参见 &lt;code&gt;Answer.GetVotersN 方法&lt;/code&gt;：&lt;a href=&#34;https://github.com/DeanThompson/zhihu-go/blob/master/answer.go&#34;&gt;zhihu-go/answer.go&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;删除节点-插入-html-导出-html&#34;&gt;删除节点、插入 HTML、导出 HTML&lt;/h3&gt;

&lt;p&gt;有一个需求是把回答内容输出到 HTML，说白了其实就是修复和清洗 HTML，具体的细节可以看 &lt;a href=&#34;https://github.com/DeanThompson/zhihu-go/blob/master/answer.go#L222&#34;&gt;answer.go 里的 answerSelectionToHtml 函数&lt;/a&gt;. 其中用到了一些需要修改文档的操作。&lt;/p&gt;

&lt;p&gt;比如，调用 &lt;code&gt;Remove()&lt;/code&gt; 方法把一个节点删掉：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;sel.Find(&amp;quot;noscript&amp;quot;).Each(func(_ int, tag *goquery.Selection) {
    tag.Remove() // 把无用的 noscript 去掉
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在节点后插入一段 HTML:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;sel.Find(&amp;quot;img&amp;quot;).Each(func(_ int, tag *goquery.Selection) {
    var src string
    if tag.HasClass(&amp;quot;origin_image&amp;quot;) {
        src, _ = tag.Attr(&amp;quot;data-original&amp;quot;)
    } else {
        src, _ = tag.Attr(&amp;quot;data-actualsrc&amp;quot;)
    }
    tag.SetAttr(&amp;quot;src&amp;quot;, src)
    if tag.Next().Size() == 0 {
        tag.AfterHtml(&amp;quot;&amp;lt;br&amp;gt;&amp;quot;)   // 在 img 标签后插入一个换行
    }
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在标签尾部 append 一段内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;wrapper := `&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;`
doc, _ := goquery.NewDocumentFromReader(strings.NewReader(wrapper))
doc.Find(&amp;quot;body&amp;quot;).AppendSelection(sel)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终输出为 html 文档：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;html, err := doc.Html()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;上面的例子基本涵盖了 zhihu-go 中关于 HTML 操作的场景，得益于 goquery 和 jQuery 的 API 风格，实现起来还是非常简单的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zhihu-go: 知乎非官方 API库 with Go</title>
      <link>http://liyangliang.me/posts/2016/03/zhihu-go</link>
      <pubDate>Mon, 28 Mar 2016 23:35:58 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2016/03/zhihu-go</guid>
      <description>&lt;p&gt;我是知乎重度用户，每天都会花点时间在知乎上面看点东西。有段时间时间线里经常出现爬虫相关的话题，也看到不少直接爬知乎信息的项目；其中一个就是 &lt;a href=&#34;https://github.com/egrcc/zhihu-python&#34;&gt;zhihu-python&lt;/a&gt;. 实际上 zhihu-python 不是一个完整的爬虫，正如其文档说明的那样，是一个 API 库，可以基于这些 API 实现一个爬虫应用。zhihu-python 实现了用户、问题、答案、收藏夹相关的信息获取类 API，对于大多数信息获取的目的已经足够。这个项目很受欢迎，然而说实话，代码质量一般，不过思路值得借鉴。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;恰巧我也是一个 &lt;a href=&#34;http://golang.org/&#34;&gt;Go 语言&lt;/a&gt; 爱好者，在之前的工作中也用 Go 写过项目。语法简单，开发效率高，性能很好。GitHub 上搜了一下，zhihu-python 或同类项目，并没有一个 Go 实现的版本。于是就想动手造个轮子，把 zhihu-python 移植到 Go，所以就有了标题里提到的 &lt;a href=&#34;https://github.com/DeanThompson/zhihu-go&#34;&gt;zhihu-go&lt;/a&gt;. 主要是出于练手的目的，最近一年都在做 Python 开发，Go 还有点生疏了。因为是移植，最初的设计和实现思路很大程度上参考或模仿了 zhihu-python，后来在开发过程中，新增了一些 API，也删除了少数几个我认为没什么用的 API. 开发过程中又看到了一个 Python 3 版本的实现 &lt;a href=&#34;https://github.com/7sDream/zhihu-py3&#34;&gt;zhihu-py3&lt;/a&gt;，这个库也是受启发于 zhihu-python，代码质量也要更好，而且实现了更丰富的 API，尤其是关于操作类的，如点赞、收藏答案等。zhihu-go 也参考了 zhihu-py3 的一些 API 设计。截止到现在，zhihu-go 的完成度应该和 zhihu-python 差不多，还多了一些 API；比 zhihu-py3 少了活动、评论及操作类的 API，这些在 TODO list 都列了出来。&lt;/p&gt;

&lt;p&gt;前几天在 V2EX 发了一个 &lt;a href=&#34;http://v2ex.com/t/266372&#34;&gt;推广的帖子&lt;/a&gt;，没想到反响还不错，收到不少支持和鼓励，GitHub 也在一两天内收获了 50 来个 star （个人最高）. 其实在这之前，还在 StuduGolang 发了 &lt;a href=&#34;http://studygolang.com/topics/1528&#34;&gt;一个主题&lt;/a&gt;，然而并没有人回复，带到 GitHub 的流量也很少，好像只有 1 个 star. golangtc 也发了，流量就更少了，可以忽略不计。&lt;/p&gt;

&lt;p&gt;后续有时间简单分析一下源码，分享一下开发过程和心得。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>极光推送 Go SDK</title>
      <link>http://liyangliang.me/posts/2015/11/jpush-api-go-client</link>
      <pubDate>Sat, 28 Nov 2015 22:32:35 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2015/11/jpush-api-go-client</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.jpush.cn/&#34;&gt;极光推送&lt;/a&gt; 是国内最早的第三方消息推送服务，官方提供了多种语言的 SDK 和 REST API，详情见 &lt;a href=&#34;http://docs.jpush.io/server/server_overview/&#34;&gt;官方文档&lt;/a&gt;。遗憾的是缺少一个 Go 语言版本的 SDK，于是我就动手造轮子，封装了一个 Go 的版本。&lt;/p&gt;

&lt;p&gt;实际上这个项目在今年 3 月份就完成了主要的推送相关的接口，在 GitHub 上也收获了几个 star 和 fork. 最近今天突然兴起，又翻出来把 device, tag, alias, report 的一些相关接口也封装完成了。&lt;/p&gt;

&lt;p&gt;啰嗦了一大推，差点忘了最重要的东西，下面给出链接：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;源代码和示例：&lt;a href=&#34;https://github.com/DeanThompson/jpush-api-go-client&#34;&gt;https://github.com/DeanThompson/jpush-api-go-client&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;官方文档：&lt;a href=&#34;http://docs.jpush.io/server/rest_api_v3_push/&#34;&gt;http://docs.jpush.io/server/rest_api_v3_push/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;欢迎使用，并 &lt;a href=&#34;https://github.com/DeanThompson/jpush-api-go-client/issues&#34;&gt;反馈 issues&lt;/a&gt; 或 &lt;a href=&#34;https://github.com/DeanThompson/jpush-api-go-client/pulls&#34;&gt;创建 pull request&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>在 Flask 项目中使用 Celery</title>
      <link>http://liyangliang.me/posts/2015/11/using-celery-with-flask</link>
      <pubDate>Sat, 14 Nov 2015 16:57:03 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2015/11/using-celery-with-flask</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://liyangliang.me/posts/2015/11/a-introduction-to-celery/&#34;&gt;前一篇 Blog&lt;/a&gt; 简单介绍了 Celery 及其用法，现在我们看看在 Flask 项目中如何使用 Celery.&lt;/p&gt;

&lt;p&gt;注意，这篇 Blog 严重参考了这两篇文章：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.miguelgrinberg.com/post/using-celery-with-flask&#34;&gt;Using Celery With Flask&lt;/a&gt;: 写了一个完整而且有意义的例子来展示如何在 Flask 中使用 Celery.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.miguelgrinberg.com/post/celery-and-the-flask-application-factory-pattern&#34;&gt;Celery and the Flask Application Factory Pattern&lt;/a&gt;: 是上文的姊妹篇，描述的是更为真实的场景下，Celery 与 &lt;a href=&#34;http://flask.pocoo.org/docs/0.10/patterns/appfactories/&#34;&gt;Flask Application Factory&lt;/a&gt; 的结合使用。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;minimum-example&#34;&gt;Minimum Example&lt;/h2&gt;

&lt;p&gt;Celery 的一些设计和概念，与 Flask 很像，在 Flask 项目中集成 Celery 也很简单，不像 Django 或其他框架需要扩展插件。首先来看个最简单的例子 example.py：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import uuid

from flask import Flask, request, jsonify
from celery import Celery

app = Flask(__name__)
app.config[&#39;CELERY_BROKER_URL&#39;] = &#39;redis://localhost:6379/0&#39;
app.config[&#39;CELERY_RESULT_BACKEND&#39;] = &#39;redis://localhost:6379/0&#39;

celery = Celery(app.name, broker=app.config[&#39;CELERY_BROKER_URL&#39;])
celery.conf.update(app.config)


@celery.task
def send_email(to, subject, content):
    return do_send_email(to, subject, content)


@app.route(&#39;/password/forgot/&#39;, methods=[&#39;POST&#39;])
def reset_password():
    email = request.form[&#39;email&#39;]
    token = str(uuid.uuid4())
    content = u&#39;请点击链接重置密码：http://example.com/password/reset/?token=%s&#39; % token
    send_email.delay(email, content)
    return jsonify(code=0, message=u&#39;发送成功&#39;)


if __name__ == &#39;__main__&#39;:
    app.run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动 Celery worker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ celery worker -A example.celery -l INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动 Web server:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python example.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，实际应用在生产环境下，不能直接用 Flask 自带的 server，需要使用 Gunicorn 这样的 WSGI 容器，或 uWSGI. 而且 Celery worker 进程和 Web server 进程应该用 supervisord 管理起来。&lt;/p&gt;

&lt;h2 id=&#34;becoming-bigger&#34;&gt;Becoming Bigger&lt;/h2&gt;

&lt;p&gt;这是个最简单的例子，实际应用会比这个复杂很多：有很多模块，更复杂的配置，更多的 task 等。在这种情况下，Flask 推荐使用 &lt;a href=&#34;http://flask.pocoo.org/docs/0.10/patterns/appfactories/&#34;&gt;Application Factory Pattern&lt;/a&gt;，也就是定义一个 function，在这里创建 Flask app 对象，并且处理注册路由（blueprints）、配置 logging 等一系列初始化操作。&lt;/p&gt;

&lt;p&gt;下面我们看看在更大的 Flask 项目里，应该如何使用 Celery.&lt;/p&gt;

&lt;h3 id=&#34;项目结构&#34;&gt;项目结构&lt;/h3&gt;

&lt;p&gt;首先来看一下整个项目的结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── README.md
├── app
│   ├── __init__.py
│   ├── config.py
│   ├── forms
│   ├── models
│   ├── tasks
│   │   ├── __init__.py
│   │   └── email.py
│   └── views
│   │   ├── __init__.py
│   │   └── account.py
├── celery_worker.py
├── manage.py
└── wsgi.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个图里省略了很多细节，简单解释一下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;项目的根目录下，有个 &lt;code&gt;celery_worker.py&lt;/code&gt; 的文件，这个文件的作用类似于 &lt;code&gt;wsgi.py&lt;/code&gt;，是启动 Celery worker 的入口。&lt;/li&gt;
&lt;li&gt;app 包里是主要业务代码，其中 tasks 里定义里一系列的 task，提供给其他模块调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;主要代码&#34;&gt;主要代码。&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;app/config.py&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class BaseConfig(object):
    CELERY_BROKER_URL = &#39;redis://localhost:6379/2&#39;
    CELERY_RESULT_BACKEND = &#39;redis://localhost:6379/2&#39;
    CELERY_TASK_SERIALIZER = &#39;json&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;BaseConfig&lt;/code&gt; 是整个项目用到的配置的基类，实际上还会派生出 &lt;code&gt;DevelopmentConfig&lt;/code&gt;, &lt;code&gt;StagingConfig&lt;/code&gt; 和 &lt;code&gt;ProductionConfig&lt;/code&gt; 等类。这里不讨论配置的细节，也只关心和 Celery 相关的配置项。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;app/__init__.py&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from celery import Celery
from flask import Flask

from app.config import BaseConfig

celery = Celery(__name__, broker=BaseConfig.CELERY_BROKER_URL)


def create_app():
    app = Flask(__name__)
    # ....
    celery.conf.update(app.config)	# 更新 celery 的配置
    # ...
    return app
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;app/tasks/email.py&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flask import current_app
from celery.utils.log import get_task_logger

from app import celery

logger = get_task_logger(__name__)


@celery.task
def send_email(to, subject, content):
    app = current_app._get_current_object()
    subject = app.config[&#39;EMAIL_SUBJECT_PREFIX&#39;] + subject
    logger.info(&#39;send message &amp;quot;%s&amp;quot; to %s&#39;, content, to)
    return do_send_email(to, subject, content)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;app/views/account.py&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import uuid

from flask import Blueprint, request,jsonify

from app.tasks.email import send_email

bp_account = Blueprint(&#39;account&#39;, __name__)


@bp_account.route(&#39;/password/forgot/&#39;, methods=[&#39;POST&#39;])
def reset_password():
    email = request.form[&#39;email&#39;]
    token = str(uuid.uuid4())
    content = u&#39;请点击链接重置密码：http://example.com/password/reset/?token=%s&#39; % token
    send_email.delay(email, content)
    return jsonify(code=0, message=u&#39;发送成功&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;ceelry_worker.py&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from app import create_app, celery

app = create_app()
app.app_context().push()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 &lt;code&gt;celery_worker.py&lt;/code&gt; 文件有两个操作：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;创建一个 Flask 实例&lt;/li&gt;
&lt;li&gt;推入 Flask application context&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第一个操作很简单，其实也是初始化了 celery 实例。&lt;/p&gt;

&lt;p&gt;第二个操作看起来有些奇怪，实际上也很好理解。如果用过 Flask 就应该知道 Flask 的 &lt;a href=&#34;http://flask.pocoo.org/docs/0.10/appcontext/&#34;&gt;Application Context&lt;/a&gt; 和 &lt;a href=&#34;http://flask.pocoo.org/docs/0.10/reqcontext/&#34;&gt;Request Context&lt;/a&gt;. Flask 一个很重要的设计理念是：在一个 Python 进程里可以运行多个应用（application），当存在多个 application 时可以通过 &lt;code&gt;current_app&lt;/code&gt; 获取当前请求所对应的 application. &lt;code&gt;current_app&lt;/code&gt; 绑定的是当前 request 的 application 的引用，在非 request-response 环境里，是没有 request context 的，所以调用 &lt;code&gt;current_app&lt;/code&gt; 就会抛出异常（&lt;code&gt;RuntimeError: working outside of application context&lt;/code&gt;）。创建一个 request context 没有必要，而且消耗资源，所以就引入了 application context.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;app.app_context().push()&lt;/code&gt; 会推入一个 application context，后续所有操作都会在这个环境里执行，直到进程退出。因此，如果在 tasks 里用到了 &lt;code&gt;current_app&lt;/code&gt; 或其它需要 application context 的东西，就一定需要这样做。（默认情况下 Celery 的 pool 是 prefork，也就是多进程，现在这种写法没有问题；但是如果指定使用 gevent，是没用的。这种情况下有别的解决方案，以后会写文章讨论。）&lt;/p&gt;

&lt;h3 id=&#34;运行&#34;&gt;运行&lt;/h3&gt;

&lt;p&gt;在项目的根路径下启动 Celery worker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ celery worker -A celery_worker.celery -l INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;上面两个例子，实际上主要的差别就是初始化方式和模块化，还有需要注意 Flask 的 application context 问题。文章内容比较简单，文中的一些链接是很好的扩展和补充，值得一看。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Celery 使用简介</title>
      <link>http://liyangliang.me/posts/2015/11/a-introduction-to-celery</link>
      <pubDate>Sat, 14 Nov 2015 16:44:34 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2015/11/a-introduction-to-celery</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;h3 id=&#34;分布式任务队列&#34;&gt;分布式任务队列&lt;/h3&gt;

&lt;p&gt;Celery 是一个分布式任务队列，下面是 &lt;a href=&#34;http://www.celeryproject.org/&#34;&gt;官网&lt;/a&gt; 的一段描述：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Celery is an asynchronous task queue/job queue based on distributed message passing.  It is focused on real-time operation, but supports scheduling as well.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Celery 简单、灵活、可靠，是一个专注于实时处理的任务队列，同时也支持任务调度。&lt;/p&gt;

&lt;h3 id=&#34;何为任务队列&#34;&gt;何为任务队列？&lt;/h3&gt;

&lt;p&gt;摘自 Celery 官方文档的 &lt;a href=&#34;http://docs.jinkan.org/docs/celery/getting-started/introduction.html&#34;&gt;中文翻译&lt;/a&gt;：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;任务队列是一种在线程或机器间分发任务的机制。&lt;/p&gt;

&lt;p&gt;消息队列的输入是工作的一个单元，称为任务，独立的职程（Worker）进程持续监视队列中是否有需要处理的新任务。&lt;/p&gt;

&lt;p&gt;Celery 用消息通信，通常使用中间人（Broker）在客户端和职程间斡旋。这个过程从客户端向队列添加消息开始，之后中间人把消息派送给职程。&lt;/p&gt;

&lt;p&gt;Celery 系统可包含多个职程和中间人，以此获得高可用性和横向扩展能力。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&#34;适用场景&#34;&gt;适用场景&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;可以在 Request-Response 循环之外执行的操作：发送邮件、推送消息&lt;/li&gt;
&lt;li&gt;耗时的操作：调用第三方 API、视频处理（前端通过 AJAX 展示进度和结果）&lt;/li&gt;
&lt;li&gt;周期性任务：取代 crontab&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;simple-tutorial&#34;&gt;Simple Tutorial&lt;/h2&gt;

&lt;p&gt;主要参考了官网文档：&lt;a href=&#34;http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html&#34;&gt;First Steps with Celery&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;选择-broker&#34;&gt;选择 Broker&lt;/h3&gt;

&lt;p&gt;下图描述了 Celery 的基本架构和工作流程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+------+      +--------+      +----------------+      +--------------+
| User | ---&amp;gt; | Broker | ---&amp;gt; | Workers (1..N) | ---&amp;gt; | Result Store |
+------+      +--------+      +----------------+      +--------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如前文所述，Celery 用消息通信。常用的 Broker 有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RabbitMQ&lt;/strong&gt;: RabbitMQ 功能完备、稳定，是一个非常可靠的选择，Celery 官网的评价是 &amp;ldquo;excellent choice for a production environment&amp;rdquo;. 缺点是使用起来毕竟有些复杂。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Redis&lt;/strong&gt;: Redis 同样功能完备，与 RabbitMQ 相比，缺点是可能因为掉电或异常退出导致数据丢失，优点是使用简单。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据库&lt;/strong&gt;: 能方便的集成 SQLAlchemy 和 Django ORM，缺点是性能差，但如果项目本来就用到了数据库，使用起来也非常便利，而且不需要再安装 RabbitMQ 或 Redis.&lt;/li&gt;
&lt;li&gt;其它: 比如 MongoDB, Amazon SQS 还有 IronMQ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们在这里选择使用 Reids.&lt;/p&gt;

&lt;h3 id=&#34;安装&#34;&gt;安装&lt;/h3&gt;

&lt;p&gt;Celery 是一个 Python 的应用，而且已经上传到了 PyPi，所以可以使用 &lt;code&gt;pip&lt;/code&gt; 或 &lt;code&gt;easy_install&lt;/code&gt; 安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install celery
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装完成后会在 PATH （或 virtualenv 的 bin 目录）添加几个命令：celery, celerybeat, celeryd 和 celeryd-multi. 我们这里只使用 celery 命令。&lt;/p&gt;

&lt;h3 id=&#34;创建-application-和-task&#34;&gt;创建 Application 和 Task&lt;/h3&gt;

&lt;p&gt;Celery 的使用方法和 Flask 很像，实例化一个 Celery 对象 &lt;code&gt;app&lt;/code&gt;，然后通过 &lt;code&gt;@app.task&lt;/code&gt; 装饰器注册一个 task. 下面是一个简单的例子 tasks.py：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from celery import Celery

app = Celery(__name__, broker=&#39;redis://localhost:6379/0&#39;)


@app.task
def add(x, y):
    return x + y
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;运行-worker&#34;&gt;运行 worker&lt;/h3&gt;

&lt;p&gt;在 tasks.py 文件所在目录运行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ celery worker -A tasks.app -l INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个命令会开启一个在前台运行的 worker，解释这个命令的意义：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;worker: 运行 worker 模块&lt;/li&gt;
&lt;li&gt;-A: &amp;ndash;app=APP, 指定使用的 Celery 实例，类似 Gunicorn 的用法&lt;/li&gt;
&lt;li&gt;-l: &amp;ndash;loglevel=INFO, 指定日志级别，可选：DEBUG, INFO, WARNING, ERROR, CRITICAL, FATAL&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其它常用的选项：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;-P: &amp;ndash;pool=prefork, 并发模型，可选：prefork (默认，multiprocessing), eventlet, gevent, threads.&lt;/li&gt;
&lt;li&gt;-c: &amp;ndash;concurrency=10, 并发级别，prefork 模型下就是子进程数量，默认等于 CPU 核心数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整的命令行选项可以这样查看：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ celery worker --help
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;调用-task&#34;&gt;调用 task&lt;/h3&gt;

&lt;p&gt;有些 Task 可以当作一个普通的函数同步调用，这里讨论异步的方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tasks import add

add.delay(1, 2)
add.apply_async(args=(1, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面两种调用方式等价，&lt;code&gt;delay()&lt;/code&gt; 方法是 &lt;code&gt;apply_async()&lt;/code&gt; 方法的简写。这个调用会把 &lt;code&gt;add&lt;/code&gt; 操作放入到队列里，然后立即返回一个 &lt;code&gt;AsyncResult&lt;/code&gt; 对象。如果关心处理结果，需要给 &lt;code&gt;app&lt;/code&gt; 配置 &lt;code&gt;CELERY_RESULT_BACKEND&lt;/code&gt;，指定一个存储后端保存任务的返回值。&lt;/p&gt;

&lt;h3 id=&#34;配置&#34;&gt;配置&lt;/h3&gt;

&lt;p&gt;前文说过 Celery 与 Flask 的使用很像，配置也是如此。一般情况下，使用 Celery 的默认配置就已经足够，但 Celery 也提供了很灵活的配置。下面是两种配置方式，&lt;a href=&#34;http://docs.celeryproject.org/en/latest/configuration.html&#34;&gt;官方文档&lt;/a&gt; 可以查看所有的配置项及默认值。&lt;/p&gt;

&lt;h4 id=&#34;直接修改配置&#34;&gt;直接修改配置&lt;/h4&gt;

&lt;p&gt;单个：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.conf.CELERY_TASK_SERIALIZER = &#39;json&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或批量（支持 &lt;code&gt;dict&lt;/code&gt; 语法）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.conf.update(
    CELERY_TASK_SERIALIZER=&#39;json&#39;,
    CELERY_ACCEPT_CONTENT=[&#39;json&#39;],  # Ignore other content
    CELERY_RESULT_SERIALIZER=&#39;json&#39;,
    CELERY_TIMEZONE=&#39;Europe/Oslo&#39;,
    CELERY_ENABLE_UTC=True
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;配置模块&#34;&gt;配置模块&lt;/h4&gt;

&lt;p&gt;类似 Flask，对于比较大的 Celery 项目，配置模块（configuration module）是更好的选择。Celery 对象有个 &lt;code&gt;config_from_object&lt;/code&gt; 方法，读取一个 object (py 文件或 class)来更新配置。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;BROKER_URL = &#39;redis://localhost:6379/0&#39;

CELERY_TASK_SERIALIZER = &#39;json&#39;
CELERY_RESULT_SERIALIZER = &#39;json&#39;
CELERY_ACCEPT_CONTENT=[&#39;json&#39;]
CELERY_TIMEZONE = &#39;Europe/Oslo&#39;
CELERY_ENABLE_UTC = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把上面的内容保存为 &lt;code&gt;celeryconfig.py&lt;/code&gt; 文件，然后：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.config_from_object(&#39;celeryconfig&#39;)
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Tornado 和 Flask 应用缓存响应结果</title>
      <link>http://liyangliang.me/posts/2015/11/cache-response-in-tornado-and-flask</link>
      <pubDate>Thu, 05 Nov 2015 16:52:12 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2015/11/cache-response-in-tornado-and-flask</guid>
      <description>&lt;p&gt;写 API 的时候，总是会想着如何能提升性能。在一般的 Web 应用里，基本上没什么 CPU 密集型的计算，大部分时间还是消耗在 IO 上面：查询数据库、读写文件、调用第三方 API 等。有些可以异步的操作，比如发送注册邮件、手机验证码等，可以用任务队列来处理。在 Python 的生态里，Celery 就是一个很成熟的解决方案。但是对于很多查询请求，还是需要同步返回的。&lt;/p&gt;

&lt;p&gt;如果真的遇到性能问题，正确的做法是先找出性能瓶颈，然后对症下药。比如优化数据库索引、优化数据库查询语句、优化算法和数据结构，加速查询和计算。但是最快的计算就是不算——或只计算一次，也就是把计算（查询）的结果缓存起来，以后相同条件的计算（查询）直接从缓存里获取，而不需要重新计算（查询）。&lt;/p&gt;

&lt;p&gt;对于耗时的计算，缓存是一种非常有效的优化手段。但缓存也不是万能的，引入缓存的同时，一些其他问题或需要注意的事情也随之而来，比如数据同步、缓存失效、命中率、分布式等。这里不深入探讨这些问题，仅针对下面这种场景，使用缓存来优化 API 性能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GET 查询&lt;/li&gt;
&lt;li&gt;查询很耗时&lt;/li&gt;
&lt;li&gt;相同条件、不同时间（或某段时间内）的查询结果是一致的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如获取静态页面（也可以通过 Nginx 直接返回），查询某些元数据列表（如国家列表、产品分类等）。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;基本思想&#34;&gt;基本思想&lt;/h2&gt;

&lt;p&gt;“一码胜千言”，直接上代码描述一下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def cachable_get(kwargs, on_cache_missing, timeout=300):
    key = make_key(kwargs)	# 计算出一个 key
    value = cache.get(key)	# 查询缓存
    if not value:
        value = on_cache_missing(kwargs)	# 缓存没有命中，计算一次
        cache.set(key, value, timeout)	# 把计算结果写入缓存
    return value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上也就是：先查缓存，如果有缓存没命中，再计算并把结果写入缓存。这种机制类似于中间件，或 Python 里的装饰器。&lt;/p&gt;

&lt;h2 id=&#34;tornado-的实现&#34;&gt;Tornado 的实现&lt;/h2&gt;

&lt;p&gt;Tornado 的 &lt;code&gt;tornado.web.RequestHandler&lt;/code&gt; 有两个方法：&lt;code&gt;prepare&lt;/code&gt; 和 &lt;code&gt;write&lt;/code&gt;。前者会在执行业务代码前执行，后者用于写入响应结果。所以可以在 &lt;code&gt;prepare&lt;/code&gt; 里查询缓存，如果命中就直接返回。没有命中的请求会执行业务代码，然后在 &lt;code&gt;write&lt;/code&gt; 里顺便写入缓存。&lt;/p&gt;

&lt;p&gt;在 Tornado 项目里，通常的做法是从 &lt;code&gt;tornado.web.RequestHandler&lt;/code&gt; 派生一个 &lt;code&gt;BaseHandler&lt;/code&gt; 用于项目内 Handler 的统一基类，方便在 &lt;code&gt;BaseHandler&lt;/code&gt; 里做一些统一的处理。如果在 &lt;code&gt;BaseHandler&lt;/code&gt; 的 &lt;code&gt;prepare&lt;/code&gt; 和 &lt;code&gt;write&lt;/code&gt; 方法实现缓存机制，会影响到所有子类的表现，这样可控性和扩展性就会差一点。推荐的做法是用 Mixin.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-

try:
    import cPickle as pickle
except ImportError:
    import pickle

import functools
from hashlib import sha1


class CacheMixin(object):
    @property
    def cache(self):
        return self.application.cache

    def _generate_key(self):
        key = pickle.dumps((self.request.path, self.request.arguments))
        return self._with_prefix(sha1(key).hexdigest())

    def _with_prefix(self, key):
        return &#39;%s:%s&#39; % (self.request.path.strip(&#39;/&#39;), key)

    def write_cache(self, chunk):
        super(CacheMixin, self).write(chunk)

    def prepare(self):
        super(CacheMixin, self).prepare()
        key = self._generate_key()
        cached = self.cache.get(key)
        if cached is not None:
            self.write_cache(pickle.loads(cached))
            self.finish()

    def write(self, chunk):
        key = self._generate_key()
        expiration = getattr(self, &#39;expiration&#39;, 300)
        self.cache.set(key, pickle.dumps(chunk), expiration)
        super(CacheMixin, self).write(chunk)


def set_cache_timeout(expiration=300):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(handler, *args, **kwargs):
            handler.expiration = expiration
            return func(handler, *args, **kwargs)

        return wrapper

    return decorator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;CacheMixin&lt;/code&gt; 在定义 Handler 时作为基类传入，覆盖 &lt;code&gt;tornado.web.RequestHandler&lt;/code&gt; 的 &lt;code&gt;prepare&lt;/code&gt; 和 &lt;code&gt;write&lt;/code&gt;，实现缓存机制。&lt;code&gt;self.application.cache&lt;/code&gt; 意味着初始化 &lt;code&gt;tornado.web.Application&lt;/code&gt; 时需要配置一个 &lt;code&gt;cache&lt;/code&gt; 属性，至少需要实现 &lt;code&gt;get&lt;/code&gt; 和支持超时的 &lt;code&gt;set&lt;/code&gt; 方法。常见的是定义一个 &lt;code&gt;CacheBackend&lt;/code&gt; 和一套 &lt;code&gt;get&lt;/code&gt;, &lt;code&gt;set&lt;/code&gt; 接口，然后封装不同的缓存实现，比如 Redis，Memcache 等。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;set_cache_timeout&lt;/code&gt; 提供了自定义缓存失效时间的能力，这个装饰器不是必须的，与之等价的方式是在 Handler 的 &lt;code&gt;get&lt;/code&gt; 方法的第一行（或第一个调用 &lt;code&gt;self.write&lt;/code&gt; 语句前）加上：&lt;code&gt;self.expiration = TIMEOUT_IN_SECONDS&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;一个没什么实际意义的使用示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Class HelloHandler(CacheMixin, tornado.web.RequestHandler):
    
    @set_cache_timeout(86400)
    def get(self):
        self.write(&amp;quot;Hello world!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;flask-的实现&#34;&gt;Flask 的实现&lt;/h2&gt;

&lt;p&gt;Flask 里可以用 &lt;code&gt;before_request&lt;/code&gt; 和 &lt;code&gt;after_request&lt;/code&gt; 这两个 hooks 实现 Tornado 里覆盖 &lt;code&gt;prepare&lt;/code&gt; 和 &lt;code&gt;write&lt;/code&gt; 来缓存所有请求，具体实现大同小异。也可以用装饰器来获得更好的灵活性。&lt;/p&gt;

&lt;p&gt;在看具体实现之前，先推荐一个 Flask 的缓存扩展：&lt;a href=&#34;https://pythonhosted.org/Flask-Cache/&#34;&gt;Flask-Cache&lt;/a&gt;. Flask-Cache 基于 &lt;code&gt;werkzeug.contrib.cache&lt;/code&gt;，后者定义了一套缓存接口和实现了多种不同 Backend 的缓存实现；Flask-Cache 在此基础上针对 Flask 做了一些应用性集成以及提供了一些其他的辅助函数。&lt;/p&gt;

&lt;p&gt;下面的例子用的是 Flask-Cache，后端用 Redis，具体的配置见 Flask-Cache 的官方文档。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    import cPickle as pickle
except ImportError:
    import pickle

import hashlib
import functools

from flask import g


class cached_response(object):
    def __init__(self, timeout=300):
        self.timeout = timeout or 300

    def _generate_key(self):
        data = pickle.dumps((request.path, request.values))
        key = hashlib.sha1(data).hexdigest()
        return self._with_prefix(key)

    @staticmethod
    def _with_prefix(key):
        return &#39;%s:%s&#39; % (request.path, key)

    def __call__(self, view_func):
        @functools.wraps(view_func)
        def decorator(*args, **kwargs):
            key = self._generate_key()
            response = cache.get(key)
            if response:
                return response

            response = view_func(*args, **kwargs)

            # 允许 view 函数通过设置 g.disable_cache = True 来控制不缓存本次请求的结果
            if getattr(g, &#39;disable_cache&#39;, False):
                return response
            
            # 只缓存 200 的请求结果
            if response.status_code == 200:
                cache.set(key, response, self.timeout)
        
            return response

        return decorator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;cached_response&lt;/code&gt; 是一个基于类的装饰器实现，接受 &lt;code&gt;timeout&lt;/code&gt; 参数指定缓存失效时间。用 &lt;code&gt;request.path&lt;/code&gt; 和 &lt;code&gt;request.values&lt;/code&gt; 序列化后的哈希值来标示相同的参数的请求（与 Tornado 版本类似）。上面的实现还展现出了一些可定制性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;只缓存 StatusCode 为 200 的请求结果&lt;/li&gt;
&lt;li&gt;允许 endpoint 通过设置 &lt;code&gt;g.disable_cache = True&lt;/code&gt; 来控制不缓存&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了这两点，还可以做其他定制，比如通过请求参数传入 &lt;code&gt;nocache=1&lt;/code&gt; 来控制获取实时结果，通过设置 &lt;code&gt;g.cache_timeout = 100&lt;/code&gt; 来覆盖默认的缓存失效时间。&lt;/p&gt;

&lt;p&gt;使用起来也很简单，只需要注册一个装饰器就可以：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@app.route(&#39;/hello/&#39;)
@cached_response(86400)
def hello():
    return &amp;quot;Hello, world!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;

&lt;p&gt;上面展示了在 Tornado 和 Flask 项目里缓存请求结果的实现方法，实际使用的时候，还是要结合具体情况做定制和调整。缓存也是一把双刃剑，在享受缓存带来性能提升的同时也要注意可能引入的问题。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>用 WTForms 和装饰器做表单校验</title>
      <link>http://liyangliang.me/posts/2015/10/using-wtforms-and-decorator-to-validate-form-in-flask</link>
      <pubDate>Sat, 31 Oct 2015 01:46:10 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://liyangliang.me/posts/2015/10/using-wtforms-and-decorator-to-validate-form-in-flask</guid>
      <description>&lt;p&gt;在一个 Web 应用里，不管是为了业务逻辑的正确性，还是系统安全性，做好参数（querystring, form, json）验证都是非常必要的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/wtforms/wtforms&#34;&gt;WTForms&lt;/a&gt; 是一个非常好用而且强大的表单校验和渲染的库，提供 Form 基类用于定义表单结构（类似 ORM），内置了丰富的字段类型和校验方法，可以很方便的用来做校验。如果应用需要输出 HTML，集成到模板里也很容易。对于 JSON  API 应用，用不到渲染的功能，但是结构化的表单和校验功能依然非常有用。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;以一个注册的应用场景为例，用户输入用户名、邮箱、密码、确认密码，服务程序先检查参数然后处理登录逻辑。这几个字段都是必填的，此外还有一些额外的限制：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户名：长度在 3-20 之间&lt;/li&gt;
&lt;li&gt;邮箱：合法的邮箱格式，比如 &amp;ldquo;abc&amp;rdquo; 就不合法&lt;/li&gt;
&lt;li&gt;密码：长度在 8-20 之间，必须同时包含大小写字母&lt;/li&gt;
&lt;li&gt;确认密码：必须与密码一致&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果参数不合法，返回 400；登录逻辑略去不表。&lt;/p&gt;

&lt;p&gt;最原始的做法，就是直接在注册的接口里取出每个参数，逐个手动校验。这种做法可能的代码是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@app.route(&#39;/user/signup/&#39;, methods=[&#39;POST&#39;])
def register():
    username = request.form.get(&#39;username&#39;)
    if not username or not (3 &amp;lt;= len(username) &amp;lt;= 20):
        abort(400)
    
    email = request.form.get(&#39;email&#39;)
    if not email or not re.match(EMAIL_REGEX, email):
        abort(400)
    
    password = request.form.get(&#39;password&#39;)
    if not password:
        abort(400)
    if password == password.lower() or password == password.upper():
        abort(400)
    
    confirm_password = request.form.get(&#39;confirm_password&#39;)
    if not confirm_password or confirm_password != password:
        abort(400)
    
    # 处理注册的逻辑
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有可能是我的写法不太对，但是这样检查参数的合法性，实在不够优雅。检查参数的代码行数甚至超出了注册的逻辑，也有些喧宾夺主的感觉。可以把这些代码移出来，使得业务逻辑代码更加清晰一点。下面先用 WTForms 来改造一下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from wtforms import Form
from wtforms.fields import StringField, PasswordField
from wtforms.validators import DataRequired, Email, Length, EqualTo, ValidationError


class SignupForm(Form):
    username = StringField(validators=[DataRequired(), Length(3, 20)])
    email = StringField(validators=[DataRequired(), Email()])
    password = PasswordField(validators=[DataRequired()])
    confirm_password = PasswordField(validators=[DataRequired(), EqualTo(&#39;password&#39;)])
    
    def validate_password(self, field):
        password = field.data
        if password == password.lower() or password == passowrd.upper():
            raise ValidationError(u&#39;必须同时包含大小写字母&#39;)


@app.route(&#39;/user/signup/&#39;, methods=[&#39;POST&#39;])
def register():
    form = SignupForm(formdata=request.form)
    if not form.validate():
        abort(400)
    
    # 处理注册逻辑，参数从 form 对象获取，比如
    username = form.username.data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个版本带来的好处很明显：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;参数更加结构化了，所有字段名和类型一目了然&lt;/li&gt;
&lt;li&gt;有内置的，语义清晰的校验方法，可以组合使用&lt;/li&gt;
&lt;li&gt;还能自定义额外的校验方法，方法签名是 &lt;code&gt;def validate_xx(self, field)&lt;/code&gt;，其中 &lt;code&gt;xx&lt;/code&gt; 是字段名，通过 &lt;code&gt;field.data&lt;/code&gt; 来获取输入的值&lt;/li&gt;
&lt;li&gt;还有没体现出来的，就是丰富的错误提示信息，既有内置的，也可以自定义&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;再看原来的 &lt;code&gt;register&lt;/code&gt; 方法，代码变得更加简洁和清晰，整体的编码质量也得到了提升。&lt;/p&gt;

&lt;p&gt;那么再考虑一下更复杂的场景，在一个返回 JSON 的 API 应用里，有很多 API，有不同的参数提交方式（GET 方法通过 query string，POST 方法可能有 form 和 JSON），一样的校验错误处理方式（abort(400) 或其他）。我们依然可以像上面那样处理，但如果再借助装饰器改进一下，又能少写几行“重复”的代码。&lt;/p&gt;

&lt;p&gt;需要注意的是，WTForms 的 formdata 支持的是类似 Werkzeug/Django/WebOb 中的 &lt;code&gt;MultiDict&lt;/code&gt; 的数据结构。Flask 中的 &lt;code&gt;request.json&lt;/code&gt; 是一个 &lt;code&gt;dict&lt;/code&gt; 类型，所以需要先包装一下。&lt;/p&gt;

&lt;p&gt;继续改造注册的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import functools

from werkzeug.datastructures import MultiDict


def validate_form(form_class):
    def decorator(view_func):
        @functools.wraps(view_func)
        def inner(*args, **kwargs):
            if request.method == &#39;GET&#39;:
                formdata = request.args
            else:
                if request.json:
                    formdata = MultiDict(request.json)
                else:
                    formdata = request.form
                    
            form = form_class(formdata=formdata)
            if not form.validate():
                return jsonify(code=400, message=form.errors), 400

            g.form = form
            return view_func(*args, **kwargs)

        return inner

    return decorator


@app.route(&#39;/user/signup/&#39;, methods=[&#39;POST&#39;])
@validate_form(form_class=SignupForm)
def register():
    form = g.form   # 运行到这里，说明表单是合法的

    # 处理注册逻辑，参数从 form 对象获取，比如
    username = form.username.data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现了一个叫 &lt;code&gt;validate_form&lt;/code&gt; 的装饰器，指定一个 Form 类，处理统一的参数获取、校验和错误处理，如果一切正确，再把 Form 对象保存到全局变量 &lt;code&gt;g&lt;/code&gt; 里面，这样就可以在 view 函数里取出来用了。现在的 &lt;code&gt;register&lt;/code&gt; 方法变得更加简洁，甚至都看不到检查参数的那些代码，只需要关心具体的和注册相关的逻辑本身就好。&lt;/p&gt;

&lt;p&gt;这个装饰器的可重用性非常好，其他的接口只要定义一个 Form 类，然后调用一下装饰器，再从 &lt;code&gt;g&lt;/code&gt; 获取 Form 对象。不仅省了很多心思和体力劳动，代码也变得更加清晰优雅和 Pythonic.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>

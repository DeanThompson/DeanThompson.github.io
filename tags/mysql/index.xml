<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mysql on 李林克斯</title>
    <link>http://liyangliang.me/tags/mysql/</link>
    <description>Recent content in Mysql on 李林克斯</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <managingEditor>yanglianglee@gmail.com (Yangliang Li)</managingEditor>
    <webMaster>yanglianglee@gmail.com (Yangliang Li)</webMaster>
    <lastBuildDate>Sat, 25 Feb 2017 16:16:14 +0800</lastBuildDate>
    
	<atom:link href="http://liyangliang.me/tags/mysql/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python 多进程导入数据到 MySQL</title>
      <link>http://liyangliang.me/posts/2017/02/load-data-into-mysql-using-python-multiprocessing/</link>
      <pubDate>Sat, 25 Feb 2017 16:16:14 +0800</pubDate>
      <author>yanglianglee@gmail.com (Yangliang Li)</author>
      <guid>http://liyangliang.me/posts/2017/02/load-data-into-mysql-using-python-multiprocessing/</guid>
      <description>&lt;p&gt;前段时间帮同事处理了一个把 CSV 数据导入到 MySQL 的需求。两个很大的 CSV 文件，
分别有 3GB、2100 万条记录和 7GB、3500 万条记录。对于这个量级的数据，用简单的单进程／单线程导入
会耗时很久，最终用了多进程的方式来实现。具体过程不赘述，记录一下几个要点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;批量插入而不是逐条插入&lt;/li&gt;
&lt;li&gt;为了加快插入速度，先不要建索引&lt;/li&gt;
&lt;li&gt;生产者和消费者模型，主进程读文件，多个 worker 进程执行插入&lt;/li&gt;
&lt;li&gt;注意控制 worker 的数量，避免对 MySQL 造成太大的压力&lt;/li&gt;
&lt;li&gt;注意处理脏数据导致的异常&lt;/li&gt;
&lt;li&gt;原始数据是 GBK 编码，所以还要注意转换成 UTF-8&lt;/li&gt;
&lt;li&gt;用 &lt;a href=&#34;http://click.pocoo.org/5/&#34;&gt;click&lt;/a&gt; 封装命令行工具&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的代码实现如下：&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MySQL参数：innodb_flush_log_at_trx_commit 和 sync_binlog</title>
      <link>http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</link>
      <pubDate>Sun, 02 Mar 2014 16:16:04 +0800</pubDate>
      <author>yanglianglee@gmail.com (Yangliang Li)</author>
      <guid>http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</guid>
      <description>&lt;p&gt;&lt;code&gt;innodb_flush_log_at_trx_commit&lt;/code&gt; 和 &lt;code&gt;sync_binlog&lt;/code&gt; 是 MySQL 的两个配置参数，前者是 InnoDB 引擎特有的。之所以把这两个参数放在一起讨论，是因为在实际应用中，它们的配置对于 MySQL 的性能有很大影响。&lt;/p&gt;

&lt;h2 id=&#34;1-innodb-flush-log-at-trx-commit&#34;&gt;1. innodb_flush_log_at_trx_commit&lt;/h2&gt;

&lt;p&gt;简而言之，&lt;a href=&#34;http://dev.MySQL.com/doc/refman/4.1/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit&#34;&gt;&lt;code&gt;innodb_flush_log_at_trx_commit&lt;/code&gt;&lt;/a&gt; 参数指定了 InnoDB 在事务提交后的日志写入频率。这么说其实并不严谨，且看其不同取值的意义和表现。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当 &lt;code&gt;innodb_flush_log_at_trx_commit&lt;/code&gt; 取值为 &lt;code&gt;0&lt;/code&gt; 的时候，log buffer 会 每秒写入到日志文件并刷写（flush）到磁盘。但每次事务提交不会有任何影响，也就是 log buffer 的刷写操作和事务提交操作没有关系。在这种情况下，MySQL性能最好，但如果 mysqld 进程崩溃，通常会导致最后 1s 的日志丢失。&lt;/li&gt;
&lt;li&gt;当取值为 &lt;code&gt;1&lt;/code&gt; 时，每次事务提交时，log buffer 会被写入到日志文件并刷写到磁盘。这也是默认值。这是最安全的配置，但由于每次事务都需要进行磁盘I/O，所以也最慢。&lt;/li&gt;
&lt;li&gt;当取值为 &lt;code&gt;2&lt;/code&gt; 时，每次事务提交会写入日志文件，但并不会立即刷写到磁盘，日志文件会每秒刷写一次到磁盘。这时如果 mysqld 进程崩溃，由于日志已经写入到系统缓存，所以并不会丢失数据；在操作系统崩溃的情况下，通常会导致最后 1s 的日志丢失。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面说到的「最后 1s」并不是绝对的，有的时候会丢失更多数据。有时候由于调度的问题，每秒刷写（once-per-second flushing）并不能保证 100% 执行。对于一些数据一致性和完整性要求不高的应用，配置为 &lt;code&gt;2&lt;/code&gt; 就足够了；如果为了最高性能，可以设置为 &lt;code&gt;0&lt;/code&gt;。有些应用，如支付服务，对一致性和完整性要求很高，所以即使最慢，也最好设置为 &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
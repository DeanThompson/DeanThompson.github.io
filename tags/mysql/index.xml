<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Mysql on 李林克斯 </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://deanthompson.github.io/tags/mysql/index.xml</link>
    <language>zh-CN</language>
    <author>Yangliang Li</author>
    
    <updated>Mon, 01 Jan 0001 00:00:00 UTC</updated>
    
    <item>
      <title>Python 多进程导入数据到 MySQL</title>
      <link>http://deanthompson.github.io/posts/2017/02/load-data-into-mysql-using-python-multiprocessing</link>
      <pubDate>Sat, 25 Feb 2017 16:16:14 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://deanthompson.github.io/posts/2017/02/load-data-into-mysql-using-python-multiprocessing</guid>
      <description>&lt;p&gt;前段时间帮同事处理了一个把 CSV 数据导入到 MySQL 的需求。两个很大的 CSV 文件，
分别有 3GB、2100 万条记录和 7GB、3500 万条记录。对于这个量级的数据，用简单的单进程／单线程导入
会耗时很久，最终用了多进程的方式来实现。具体过程不赘述，记录一下几个要点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;批量插入而不是逐条插入&lt;/li&gt;
&lt;li&gt;为了加快插入速度，先不要建索引&lt;/li&gt;
&lt;li&gt;生产者和消费者模型，主进程读文件，多个 worker 进程执行插入&lt;/li&gt;
&lt;li&gt;注意控制 worker 的数量，避免对 MySQL 造成太大的压力&lt;/li&gt;
&lt;li&gt;注意处理脏数据导致的异常&lt;/li&gt;
&lt;li&gt;原始数据是 GBK 编码，所以还要注意转换成 UTF-8&lt;/li&gt;
&lt;li&gt;用 &lt;a href=&#34;http://click.pocoo.org/5/&#34;&gt;click&lt;/a&gt; 封装命令行工具&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的代码实现如下：&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import codecs
import csv
import logging
import multiprocessing
import os
import warnings

import click
import MySQLdb
import sqlalchemy

warnings.filterwarnings(&#39;ignore&#39;, category=MySQLdb.Warning)

# 批量插入的记录数量
BATCH = 5000

DB_URI = &#39;mysql://root@localhost:3306/example?charset=utf8&#39;

engine = sqlalchemy.create_engine(DB_URI)


def get_table_cols(table):
    sql = &#39;SELECT * FROM `{table}` LIMIT 0&#39;.format(table=table)
    res = engine.execute(sql)
    return res.keys()


def insert_many(table, cols, rows, cursor):
    sql = &#39;INSERT INTO `{table}` ({cols}) VALUES ({marks})&#39;.format(
            table=table,
            cols=&#39;, &#39;.join(cols),
            marks=&#39;, &#39;.join([&#39;%s&#39;] * len(cols)))
    cursor.execute(sql, *rows)
    logging.info(&#39;process %s inserted %s rows into table %s&#39;, os.getpid(), len(rows), table)


def insert_worker(table, cols, queue):
    rows = []
    # 每个子进程创建自己的 engine 对象
    cursor = sqlalchemy.create_engine(DB_URI)
    while True:
        row = queue.get()
        if row is None:
            if rows:
                insert_many(table, cols, rows, cursor)
            break

        rows.append(row)
        if len(rows) == BATCH:
            insert_many(table, cols, rows, cursor)
            rows = []


def insert_parallel(table, reader, w=10):
    cols = get_table_cols(table)

    # 数据队列，主进程读文件并往里写数据，worker 进程从队列读数据
    # 注意一下控制队列的大小，避免消费太慢导致堆积太多数据，占用过多内存
    queue = multiprocessing.Queue(maxsize=w*BATCH*2)
    workers = []
    for i in range(w):
        p = multiprocessing.Process(target=insert_worker, args=(table, cols, queue))
        p.start()
        workers.append(p)
        logging.info(&#39;starting # %s worker process, pid: %s...&#39;, i + 1, p.pid)

    dirty_data_file = &#39;./{}_dirty_rows.csv&#39;.format(table)
    xf = open(dirty_data_file, &#39;w&#39;)
    writer = csv.writer(xf, delimiter=reader.dialect.delimiter)

    for line in reader:
        # 记录并跳过脏数据: 键值数量不一致
        if len(line) != len(cols):
            writer.writerow(line)
            continue

        # 把 None 值替换为 &#39;NULL&#39;
        clean_line = [None if x == &#39;NULL&#39; else x for x in line]

        # 往队列里写数据
        queue.put(tuple(clean_line))
        if reader.line_num % 500000 == 0:
            logging.info(&#39;put %s tasks into queue.&#39;, reader.line_num)

    xf.close()

    # 给每个 worker 发送任务结束的信号
    logging.info(&#39;send close signal to worker processes&#39;)
    for i in range(w):
        queue.put(None)

    for p in workers:
        p.join()


def convert_file_to_utf8(f, rv_file=None):
    if not rv_file:
        name, ext = os.path.splitext(f)
        if isinstance(name, unicode):
            name = name.encode(&#39;utf8&#39;)
        rv_file = &#39;{}_utf8{}&#39;.format(name, ext)
    logging.info(&#39;start to process file %s&#39;, f)
    with open(f) as infd:
        with open(rv_file, &#39;w&#39;) as outfd:
            lines = []
            loop = 0
            chunck = 200000
            first_line = infd.readline().strip(codecs.BOM_UTF8).strip() + &#39;\n&#39;
            lines.append(first_line)
            for line in infd:
                clean_line = line.decode(&#39;gb18030&#39;).encode(&#39;utf8&#39;)
                clean_line = clean_line.rstrip() + &#39;\n&#39;
                lines.append(clean_line)
                if len(lines) == chunck:
                    outfd.writelines(lines)
                    lines = []
                    loop += 1
                    logging.info(&#39;processed %s lines.&#39;, loop * chunck)

            outfd.writelines(lines)
            logging.info(&#39;processed %s lines.&#39;, loop * chunck + len(lines))


@click.group()
def cli():
    logging.basicConfig(level=logging.INFO,
                        format=&#39;%(asctime)s - %(levelname)s - %(name)s - %(message)s&#39;)


@cli.command(&#39;gbk_to_utf8&#39;)
@click.argument(&#39;f&#39;)
def convert_gbk_to_utf8(f):
    convert_file_to_utf8(f)


@cli.command(&#39;load&#39;)
@click.option(&#39;-t&#39;, &#39;--table&#39;, required=True, help=&#39;表名&#39;)
@click.option(&#39;-i&#39;, &#39;--filename&#39;, required=True, help=&#39;输入文件&#39;)
@click.option(&#39;-w&#39;, &#39;--workers&#39;, default=10, help=&#39;worker 数量，默认 10&#39;)
def load_fac_day_pro_nos_sal_table(table, filename, workers):
    with open(filename) as fd:
        fd.readline()   # skip header
        reader = csv.reader(fd)
        insert_parallel(table, reader, w=workers)


if __name__ == &#39;__main__&#39;:
    cli()
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>MySQL参数：innodb_flush_log_at_trx_commit 和 sync_binlog</title>
      <link>http://deanthompson.github.io/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog</link>
      <pubDate>Sun, 02 Mar 2014 16:16:04 CST</pubDate>
      <author>Yangliang Li</author>
      <guid>http://deanthompson.github.io/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog</guid>
      <description>&lt;p&gt;&lt;code&gt;innodb_flush_log_at_trx_commit&lt;/code&gt; 和 &lt;code&gt;sync_binlog&lt;/code&gt; 是 MySQL 的两个配置参数，前者是 InnoDB 引擎特有的。之所以把这两个参数放在一起讨论，是因为在实际应用中，它们的配置对于 MySQL 的性能有很大影响。&lt;/p&gt;

&lt;h2 id=&#34;1-innodb-flush-log-at-trx-commit&#34;&gt;1. innodb_flush_log_at_trx_commit&lt;/h2&gt;

&lt;p&gt;简而言之，&lt;a href=&#34;http://dev.MySQL.com/doc/refman/4.1/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit&#34;&gt;&lt;code&gt;innodb_flush_log_at_trx_commit&lt;/code&gt;&lt;/a&gt; 参数指定了 InnoDB 在事务提交后的日志写入频率。这么说其实并不严谨，且看其不同取值的意义和表现。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当 &lt;code&gt;innodb_flush_log_at_trx_commit&lt;/code&gt; 取值为 &lt;code&gt;0&lt;/code&gt; 的时候，log buffer 会 每秒写入到日志文件并刷写（flush）到磁盘。但每次事务提交不会有任何影响，也就是 log buffer 的刷写操作和事务提交操作没有关系。在这种情况下，MySQL性能最好，但如果 mysqld 进程崩溃，通常会导致最后 1s 的日志丢失。&lt;/li&gt;
&lt;li&gt;当取值为 &lt;code&gt;1&lt;/code&gt; 时，每次事务提交时，log buffer 会被写入到日志文件并刷写到磁盘。这也是默认值。这是最安全的配置，但由于每次事务都需要进行磁盘I/O，所以也最慢。&lt;/li&gt;
&lt;li&gt;当取值为 &lt;code&gt;2&lt;/code&gt; 时，每次事务提交会写入日志文件，但并不会立即刷写到磁盘，日志文件会每秒刷写一次到磁盘。这时如果 mysqld 进程崩溃，由于日志已经写入到系统缓存，所以并不会丢失数据；在操作系统崩溃的情况下，通常会导致最后 1s 的日志丢失。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面说到的「最后 1s」并不是绝对的，有的时候会丢失更多数据。有时候由于调度的问题，每秒刷写（once-per-second flushing）并不能保证 100% 执行。对于一些数据一致性和完整性要求不高的应用，配置为 &lt;code&gt;2&lt;/code&gt; 就足够了；如果为了最高性能，可以设置为 &lt;code&gt;0&lt;/code&gt;。有些应用，如支付服务，对一致性和完整性要求很高，所以即使最慢，也最好设置为 &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-sync-binlog&#34;&gt;2. sync_binlog&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://dev.MySQL.com/doc/refman/5.5/en/replication-options-binary-log.html#sysvar_sync_binlog&#34;&gt;sync_binlog&lt;/a&gt; 是 MySQL 的二进制日志（binary log）同步到磁盘的频率。MySQL server 在 binary log 每写入 &lt;code&gt;sync_binlog&lt;/code&gt; 次后，刷写到磁盘。&lt;/p&gt;

&lt;p&gt;如果 &lt;code&gt;autocommit&lt;/code&gt; 开启，每个语句都写一次 binary log，否则每次事务写一次。默认值是 &lt;code&gt;0&lt;/code&gt;，不主动同步，而依赖操作系统本身不定期把文件内容 flush 到磁盘。设为 &lt;code&gt;1&lt;/code&gt; 最安全，在每个语句或事务后同步一次 binary log，即使在崩溃时也最多丢失一个语句或事务的日志，但因此也最慢。&lt;/p&gt;

&lt;p&gt;大多数情况下，对数据的一致性并没有很严格的要求，所以并不会把 &lt;code&gt;sync_binlog&lt;/code&gt; 配置成 &lt;code&gt;1&lt;/code&gt;. 为了追求高并发，提升性能，可以设置为 &lt;code&gt;100&lt;/code&gt; 或直接用 &lt;code&gt;0&lt;/code&gt;. 而和 &lt;code&gt;innodb_flush_log_at_trx_commit&lt;/code&gt; 一样，对于支付服务这样的应用，还是比较推荐 &lt;code&gt;sync_binlog = 1&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
